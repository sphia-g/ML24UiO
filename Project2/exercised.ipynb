{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part d): Classification  analysis using neural networks\n",
    "\n",
    "With a well-written code it should now be easy to change the\n",
    "activation function for the output layer.\n",
    "\n",
    "Here we will change the cost function for our neural network code\n",
    "developed in parts b) and c) in order to perform a classification analysis. \n",
    "\n",
    "We will here study the Wisconsin Breast Cancer  data set. This is a typical binary classification problem with just one single output, either True or Fale, $0$ or $1$ etc.\n",
    "You find more information about this at the [Scikit-Learn\n",
    "site](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_breast_cancer.html) or at the [University of California\n",
    "at Irvine](https://archive.ics.uci.edu/ml/datasets/breast+cancer+wisconsin+(original)). \n",
    "\n",
    "To measure the performance of our classification problem we use the\n",
    "so-called *accuracy* score.  The accuracy is as you would expect just\n",
    "the number of correctly guessed targets $t_i$ divided by the total\n",
    "number of targets, that is"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\text{Accuracy} = \\frac{\\sum_{i=1}^n I(t_i = y_i)}{n} ,\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $I$ is the indicator function, $1$ if $t_i = y_i$ and $0$\n",
    "otherwise if we have a binary classification problem. Here $t_i$\n",
    "represents the target and $y_i$ the outputs of your FFNN code and $n$ is simply the number of targets $t_i$.\n",
    "\n",
    "Discuss your results and give a critical analysis of the various parameters, including hyper-parameters like the learning rates and the regularization parameter $\\lambda$ (as you did in Ridge Regression), various activation functions, number of hidden layers and nodes and activation functions.  \n",
    "\n",
    "As stated in the introduction, it can also be useful to study other\n",
    "datasets. \n",
    "\n",
    "Again, we strongly recommend that you compare your own neural Network\n",
    "code for classification and pertinent results against a similar code using **Scikit-Learn**  or **tensorflow/keras** or **pytorch**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PLAN:\n",
    "1. last ned breast cancer data set\n",
    "2. bruk ffnn til klassifikasjon (med ... som siste lag)\n",
    "3. Bruk back propogation for å forbedre svaret\n",
    "4. bruke accuracy til å teste resultatene mine "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17070\n",
      "30\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (569,2) (30,) ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[69], line 35\u001b[0m\n\u001b[0;32m     32\u001b[0m batched_layers \u001b[38;5;241m=\u001b[39m [(layers[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mT, layers[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m1\u001b[39m])]\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m##fn.create_layers(network_input_size, layer_output_sizes)\u001b[39;00m\n\u001b[1;32m---> 35\u001b[0m predict \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeed_forward_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactivation_funcs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     37\u001b[0m layer_grads \u001b[38;5;241m=\u001b[39m fn\u001b[38;5;241m.\u001b[39mbackpropagation_batch(X, layers, activation_funcs, target, activation_ders)\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28mprint\u001b[39m(layer_grads)\n",
      "File \u001b[1;32mc:\\Users\\47958\\OneDrive\\Dokumenter\\Master\\FYS-STK\\ML24UiO\\Project2\\FFNN.py:124\u001b[0m, in \u001b[0;36mfeed_forward_batch\u001b[1;34m(inputs, layers, activation_funcs)\u001b[0m\n\u001b[0;32m    122\u001b[0m a \u001b[38;5;241m=\u001b[39m inputs\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m (W, b), activation_func \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(layers, activation_funcs):\n\u001b[1;32m--> 124\u001b[0m     z \u001b[38;5;241m=\u001b[39m \u001b[43ma\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m@\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mW\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\n\u001b[0;32m    125\u001b[0m     a \u001b[38;5;241m=\u001b[39m activation_func(z)\n\u001b[0;32m    126\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m a\n",
      "\u001b[1;31mValueError\u001b[0m: operands could not be broadcast together with shapes (569,2) (30,) "
     ]
    }
   ],
   "source": [
    "## just downloading the dataset\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import autograd.numpy as np\n",
    "from autograd import grad, elementwise_grad\n",
    "import FFNN as fn\n",
    "\n",
    "wisconsin = load_breast_cancer()\n",
    "X = wisconsin.data\n",
    "target = wisconsin.target\n",
    "target = target.reshape(target.shape[0], 1)\n",
    "\n",
    "X_train, X_val, t_train, t_val = train_test_split(X, target)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "\n",
    "#print(X[0].size)\n",
    "\n",
    "#\"\"\"\"\n",
    "network_input_size = X[0].size\n",
    "print(X.size)\n",
    "print(network_input_size)\n",
    "layer_output_sizes = [2, 3, 2]\n",
    "activation_funcs = [fn.sigmoid, fn.sigmoid, fn.ReLU]\n",
    "activation_ders = [fn.sigmoid_der, fn.sigmoid_der, fn.ReLU_der]\n",
    "\n",
    "layers = [(np.random.randn(network_input_size,2), np.random.randn(network_input_size))]\n",
    "batched_layers = [(layers[0][0].T, layers[0][1])]\n",
    "##fn.create_layers(network_input_size, layer_output_sizes)\n",
    "\n",
    "predict = fn.feed_forward_batch(X, layers, activation_funcs)\n",
    "\n",
    "layer_grads = fn.backpropagation_batch(X, layers, activation_funcs, target, activation_ders)\n",
    "print(layer_grads)\n",
    "\n",
    "cost_grad = grad(fn.cost, 0)\n",
    "print(cost_grad(layers, X, activation_funcs, target))\n",
    "\n",
    "cost_grad = grad(fn.cost_batch, 0)\n",
    "print(cost_grad(batched_layers, X, activation_funcs, target))\n",
    "#\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, cost: 0.28078489039995186\n",
      "Epoch 100, cost: 0.2590700171880951\n",
      "Epoch 200, cost: 0.2323591085539164\n",
      "Epoch 300, cost: 0.20303611055866658\n",
      "Epoch 400, cost: 0.17551267807657506\n",
      "Epoch 500, cost: 0.153316116978721\n",
      "Epoch 600, cost: 0.13708704198667834\n",
      "Epoch 700, cost: 0.12555328099051946\n",
      "Epoch 800, cost: 0.11713940343342391\n",
      "Epoch 900, cost: 0.11066877003437493\n",
      "Accuracy on test set: 0.8881118881118881\n",
      "False positives:  13\n",
      "True positives:  86\n",
      "False Negatives:  3\n",
      "True negatives:  41\n"
     ]
    }
   ],
   "source": [
    "## https://gpt.uio.no/chat/810796\n",
    "import autograd.numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "import FFNN as fn\n",
    "import random\n",
    "\n",
    "np.random.seed(42) #random seed to ensure reproducibility\n",
    "# Load and preprocess the Wisconsin Breast Cancer dataset\n",
    "data = datasets.load_breast_cancer()\n",
    "X = data.data\n",
    "y = data.target\n",
    "y = y.reshape(-1, 1)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42) \n",
    "\n",
    "input_size = X_train.shape[1]\n",
    "output_layer = [1]\n",
    "hidden_layers = [10]  \n",
    "layers = fn.create_layers_batch(input_size, hidden_layers+output_layer)\n",
    "activation_funcs = [fn.ReLU, fn.ReLU]\n",
    "activation_ders = [fn.ReLU_der, fn.ReLU_der]\n",
    "# Hyperparameters\n",
    "learning_rate = 0.01\n",
    "epochs = 1000\n",
    "\n",
    "\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    grads =fn.backpropagation_batch(X_train, layers, activation_funcs, y_train, activation_ders)\n",
    "    for i, (W, b) in enumerate(layers):\n",
    "        dW, db = grads[i]\n",
    "        W -= learning_rate * dW\n",
    "        b -= learning_rate * db\n",
    "        layers[i] = (W, b)\n",
    "    \n",
    "    if epoch % 100 == 0:\n",
    "        cost_value = fn.cost_batch(layers, X_train, activation_funcs, y_train)\n",
    "        print(f\"Epoch {epoch}, cost: {cost_value}\")\n",
    "\n",
    "# Evaluate the model\n",
    "prediction = fn.feed_forward_batch(X_test, layers, activation_funcs)\n",
    "prediction = np.round(prediction) \n",
    "accuracy = accuracy_score(y_test, prediction) ## prediction is 1 or 0\n",
    "print(f\"Accuracy on test set: {accuracy}\")\n",
    "\n",
    "##confusion matrix:\n",
    "falseP = 0\n",
    "trueP = 0\n",
    "falseN = 0\n",
    "trueN = 0\n",
    "\n",
    "for i in range(prediction.size):\n",
    "    pred = prediction[i][0]\n",
    "    sol = y_test[i][0]\n",
    "    if pred==1 and sol==0 :\n",
    "        falseP += 1\n",
    "    elif pred==1 and sol==1:\n",
    "        trueP +=1\n",
    "    elif pred==0 and sol==1:\n",
    "        falseN +=1\n",
    "    else:\n",
    "        trueN +=1\n",
    "\n",
    "print(\"False positives: \", falseP )\n",
    "print(\"True positives: \", trueP)\n",
    "print(\"False Negatives: \", falseN )\n",
    "print(\"True negatives: \", trueN)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train_test_split har et parameter test_size, som er initialisert til å være 0.25. \n",
    "\n",
    "sigmpid, relu, sigmoid: lr=0.01, e=1000, test_size=0.99\n",
    "Accuracy on test set: 0.6294326241134752\n",
    "-||- test_size = 0.1\n",
    "Accuracy on test set: 0.8947368421052632\n",
    "\n",
    "relu, sigmoid, -||- test =0.1\n",
    "Accuracy on test set: 0.7017543859649122\n",
    "\n",
    "sigmoid, sigmoid -||-\n",
    "Accuracy on test set: 0.8771929824561403\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
