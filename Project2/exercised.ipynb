{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part d): Classification  analysis using neural networks\n",
    "\n",
    "With a well-written code it should now be easy to change the\n",
    "activation function for the output layer.\n",
    "\n",
    "Here we will change the cost function for our neural network code\n",
    "developed in parts b) and c) in order to perform a classification analysis. \n",
    "\n",
    "We will here study the Wisconsin Breast Cancer  data set. This is a typical binary classification problem with just one single output, either True or Fale, $0$ or $1$ etc.\n",
    "You find more information about this at the [Scikit-Learn\n",
    "site](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_breast_cancer.html) or at the [University of California\n",
    "at Irvine](https://archive.ics.uci.edu/ml/datasets/breast+cancer+wisconsin+(original)). \n",
    "\n",
    "To measure the performance of our classification problem we use the\n",
    "so-called *accuracy* score.  The accuracy is as you would expect just\n",
    "the number of correctly guessed targets $t_i$ divided by the total\n",
    "number of targets, that is"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\text{Accuracy} = \\frac{\\sum_{i=1}^n I(t_i = y_i)}{n} ,\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $I$ is the indicator function, $1$ if $t_i = y_i$ and $0$\n",
    "otherwise if we have a binary classification problem. Here $t_i$\n",
    "represents the target and $y_i$ the outputs of your FFNN code and $n$ is simply the number of targets $t_i$.\n",
    "\n",
    "Discuss your results and give a critical analysis of the various parameters, including hyper-parameters like the learning rates and the regularization parameter $\\lambda$ (as you did in Ridge Regression), various activation functions, number of hidden layers and nodes and activation functions.  \n",
    "\n",
    "As stated in the introduction, it can also be useful to study other\n",
    "datasets. \n",
    "\n",
    "Again, we strongly recommend that you compare your own neural Network\n",
    "code for classification and pertinent results against a similar code using **Scikit-Learn**  or **tensorflow/keras** or **pytorch**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PLAN:\n",
    "1. last ned breast cancer data set\n",
    "2. bruk ffnn til klassifikasjon (med ... som siste lag)\n",
    "3. Bruk back propogation for å forbedre svaret\n",
    "4. bruke accuracy til å teste resultatene mine "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## just downloading the dataset\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import autograd.numpy as np\n",
    "from autograd import grad, elementwise_grad\n",
    "import FFNN as fn\n",
    "\n",
    "wisconsin = load_breast_cancer()\n",
    "X = wisconsin.data\n",
    "target = wisconsin.target\n",
    "target = target.reshape(target.shape[0], 1)\n",
    "\n",
    "X_train, X_val, t_train, t_val = train_test_split(X, target)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "\n",
    "#print(X[0].size)\n",
    "\n",
    "#\"\"\"\"\n",
    "network_input_size = X[0].size\n",
    "print(X.size)\n",
    "print(network_input_size)\n",
    "layer_output_sizes = [2, 3, 2]\n",
    "activation_funcs = [fn.sigmoid, fn.sigmoid, fn.ReLU]\n",
    "activation_ders = [fn.sigmoid_der, fn.sigmoid_der, fn.ReLU_der]\n",
    "\n",
    "layers = [(np.random.randn(network_input_size,2), np.random.randn(network_input_size))]\n",
    "batched_layers = [(layers[0][0].T, layers[0][1])]\n",
    "##fn.create_layers(network_input_size, layer_output_sizes)\n",
    "\n",
    "predict = fn.feed_forward_batch(X, layers, activation_funcs)\n",
    "\n",
    "layer_grads = fn.backpropagation_batch(X, layers, activation_funcs, target, activation_ders)\n",
    "print(layer_grads)\n",
    "\n",
    "cost_grad = grad(fn.cost, 0)\n",
    "print(cost_grad(layers, X, activation_funcs, target))\n",
    "\n",
    "cost_grad = grad(fn.cost_batch, 0)\n",
    "print(cost_grad(batched_layers, X, activation_funcs, target))\n",
    "#\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 1 is different from 512)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 114\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;66;03m# Training loop\u001b[39;00m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[1;32m--> 114\u001b[0m     grads \u001b[38;5;241m=\u001b[39m\u001b[43mfn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackpropagation_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactivation_funcs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactivation_ders\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, (W, b) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(layers):\n\u001b[0;32m    116\u001b[0m         dW, db \u001b[38;5;241m=\u001b[39m grads[i]\n",
      "File \u001b[1;32mc:\\Users\\47958\\OneDrive\\Dokumenter\\Master\\FYS-STK\\ML24UiO\\Project2\\FFNN.py:164\u001b[0m, in \u001b[0;36mbackpropagation_batch\u001b[1;34m(inputs, layers, activation_funcs, target, activation_ders, cost_der)\u001b[0m\n\u001b[0;32m      0\u001b[0m <Error retrieving source code with stack_data see ipython/ipython#13598>\n",
      "\u001b[1;31mValueError\u001b[0m: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 1 is different from 512)"
     ]
    }
   ],
   "source": [
    "## https://gpt.uio.no/chat/810796\n",
    "import autograd.numpy as np\n",
    "from autograd import grad, elementwise_grad\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import FFNN as fn\n",
    "\n",
    "# Defining some activation functions and their derivatives\n",
    "def ReLU(z):\n",
    "    return np.where(z > 0, z, 0)\n",
    "\n",
    "def ReLU_der(z):\n",
    "    return np.where(z > 0, 1, 0)\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def sigmoid_der(z):\n",
    "    return sigmoid(z) * (1 - sigmoid(z))\n",
    "\n",
    "def mse(predict, target):\n",
    "    return np.mean((predict - target) ** 2)\n",
    "\n",
    "def mse_der(predict, target):\n",
    "    return 2 / len(target) * (predict - target)\n",
    "\n",
    "# Create the layers of the neural network\n",
    "def create_layers_batch(network_input_size, layer_output_sizes):\n",
    "    layers = []\n",
    "    i_size = network_input_size\n",
    "    for layer_output_size in layer_output_sizes:\n",
    "        W = np.random.randn(i_size, layer_output_size) \n",
    "        b = np.random.randn(layer_output_size)\n",
    "        layers.append((W, b))\n",
    "        i_size = layer_output_size\n",
    "    return layers\n",
    "\n",
    "# Feed forward function for a batch of inputs\n",
    "def feed_forward_batch(inputs, layers, activation_funcs):\n",
    "    a = inputs\n",
    "    for (W, b), activation_func in zip(layers, activation_funcs):\n",
    "        z = a @ W + b \n",
    "        a = activation_func(z)\n",
    "    return a\n",
    "\n",
    "# Cost function for a batch of inputs\n",
    "def cost_batch(layers, inputs, activation_funcs, target):\n",
    "    predict = feed_forward_batch(inputs, layers, activation_funcs)\n",
    "    return mse(predict, target)\n",
    "\n",
    "# Function to save the input at each layer during feed forward for backpropagation\n",
    "def feed_forward_saver_batch(inputs, layers, activation_funcs):\n",
    "    layer_inputs = []\n",
    "    zs = []\n",
    "    a = inputs\n",
    "    for (W, b), activation_func in zip(layers, activation_funcs):\n",
    "        layer_inputs.append(a)\n",
    "        z = a @ W + b\n",
    "        a = activation_func(z)\n",
    "        zs.append(z)\n",
    "    return layer_inputs, zs, a\n",
    "\n",
    "# Backpropagation function\n",
    "def backpropagation_batch(inputs, layers, activation_funcs, target, activation_ders, cost_der=mse_der):\n",
    "    layer_inputs, zs, predict = feed_forward_saver_batch(inputs, layers, activation_funcs)\n",
    "    layer_grads = [() for layer in layers]\n",
    "    \n",
    "    for i in reversed(range(len(layers))):\n",
    "        layer_input, z, activation_der = layer_inputs[i], zs[i], activation_ders[i]\n",
    "\n",
    "        if i == len(layers) - 1:\n",
    "            dC_da = cost_der(predict, target)\n",
    "        else:\n",
    "            (W, b) = layers[i + 1]\n",
    "            dC_da = dC_dz @ W.T\n",
    "\n",
    "        dC_dz = dC_da * activation_der(z)\n",
    "        dC_dW = layer_input.T @ dC_dz\n",
    "        dC_db = np.sum(dC_dz, axis=0)\n",
    "\n",
    "        layer_grads[i] = (dC_dW, dC_db)\n",
    "\n",
    "    return layer_grads\n",
    "\n",
    "\n",
    "# Load and preprocess the Wisconsin Breast Cancer dataset\n",
    "data = datasets.load_breast_cancer()\n",
    "X = data.data\n",
    "y = data.target\n",
    "y = y.reshape(-1, 1)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "\n",
    "# Network configuration\n",
    "input_size = X_train.shape[1]\n",
    "output_size = 1\n",
    "hidden_layers = [10, 10]  # Two hidden layers with 10 neurons each\n",
    "layers = fn.create_layers_batch(input_size, hidden_layers + [output_size])\n",
    "activation_funcs = [fn.ReLU, fn.ReLU, fn.sigmoid]\n",
    "activation_ders = [fn.ReLU_der, fn.ReLU_der, fn.sigmoid_der]\n",
    "\n",
    "# Hyperparameters\n",
    "lr = 0.01\n",
    "epochs = 1000\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    grads =fn.backpropagation_batch(X_train, layers, activation_funcs, y_train, activation_ders)\n",
    "    for i, (W, b) in enumerate(layers):\n",
    "        dW, db = grads[i]\n",
    "        W -= lr * dW\n",
    "        b -= lr * db\n",
    "        layers[i] = (W, b)\n",
    "    \n",
    "    if epoch % 100 == 0:\n",
    "        cost_value = fn.cost_batch(layers, X_train, activation_funcs, y_train)\n",
    "        print(f\"Epoch {epoch}, cost: {cost_value}\")\n",
    "\n",
    "# Evaluate the model\n",
    "preds = fn.feed_forward_batch(X_test, layers, activation_funcs)\n",
    "preds = np.round(preds)\n",
    "accuracy = accuracy_score(y_test, preds)\n",
    "print(f\"Accuracy on test set: {accuracy}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
